{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be24f6c1-0db4-423f-bd7c-0169701696e0",
   "metadata": {},
   "source": [
    "# Uncensored Open Source LLM models\n",
    "\n",
    "This notebook is a showcase of how to use local LLM and the langchain toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff082256-4383-44cd-8e5f-81cc8c64afcb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in /opt/conda/envs/rob/lib/python3.9/site-packages (0.0.174)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /opt/conda/envs/rob/lib/python3.9/site-packages (from langchain) (1.10.7)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/envs/rob/lib/python3.9/site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/envs/rob/lib/python3.9/site-packages (from langchain) (2.30.0)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/envs/rob/lib/python3.9/site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /opt/conda/envs/rob/lib/python3.9/site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/envs/rob/lib/python3.9/site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/envs/rob/lib/python3.9/site-packages (from langchain) (2.0.15)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /opt/conda/envs/rob/lib/python3.9/site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/envs/rob/lib/python3.9/site-packages (from langchain) (1.24.3)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /opt/conda/envs/rob/lib/python3.9/site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /opt/conda/envs/rob/lib/python3.9/site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/rob/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/envs/rob/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/rob/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/rob/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/rob/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/rob/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /opt/conda/envs/rob/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /opt/conda/envs/rob/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in /opt/conda/envs/rob/lib/python3.9/site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/envs/rob/lib/python3.9/site-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/rob/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/rob/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/rob/lib/python3.9/site-packages (from requests<3,>=2->langchain) (1.26.15)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/envs/rob/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /opt/conda/envs/rob/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/envs/rob/lib/python3.9/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1445c29-ae87-4ca4-9017-71a6a55fe13f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python in /opt/conda/envs/rob/lib/python3.9/site-packages (0.1.51)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/envs/rob/lib/python3.9/site-packages (from llama-cpp-python) (4.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b1680a-daeb-47ea-9996-f14f3e3b03da",
   "metadata": {},
   "source": [
    "## Select the model\n",
    "\n",
    "Select the model path. All models are located at `/mnt/HC_Volume_32195498/ai/` and are *uncensored* and have had their *bias removed*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a79099d-58a8-4130-9379-89a42774c6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model_path=\"/mnt/HC_Volume_32195498/ai/Wizard-Vicuna-13B-Uncensored-GGML/Wizard-Vicuna-13B-Uncensored.ggml.q5_0.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b9ffe75-ab7a-49c5-9d59-851e743cd93f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/conda/envs/rob/lib/python3.9/site-packages (0.27.7)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/envs/rob/lib/python3.9/site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/rob/lib/python3.9/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/conda/envs/rob/lib/python3.9/site-packages (from openai) (2.30.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/rob/lib/python3.9/site-packages (from requests>=2.20->openai) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/rob/lib/python3.9/site-packages (from requests>=2.20->openai) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/rob/lib/python3.9/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/rob/lib/python3.9/site-packages (from requests>=2.20->openai) (3.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/envs/rob/lib/python3.9/site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/envs/rob/lib/python3.9/site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/envs/rob/lib/python3.9/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/envs/rob/lib/python3.9/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/envs/rob/lib/python3.9/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/envs/rob/lib/python3.9/site-packages (from aiohttp->openai) (1.3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfea310-9bed-4f50-a14d-66a417e8fd41",
   "metadata": {},
   "source": [
    "## OpenAI LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a28568d-2456-48d1-ace8-57c5209764ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not import azure.core python package.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "import os\n",
    "\n",
    "openai_llm = OpenAI(openai_api_key=os.environ[\"OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45506a61-6111-41e7-8f37-3260d32b542e",
   "metadata": {},
   "source": [
    "## Select the seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e67ed35-c618-4280-80ff-c40ca18b5275",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_seed = 99945383"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e0f7be-13df-41a9-89e0-1a9cc0906a56",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9955f273-c1a1-4b6f-a5d8-dee57baebc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536c1d72-a212-431f-83b4-5afc95bb6a6c",
   "metadata": {},
   "source": [
    "## Build the prompt\n",
    "\n",
    "Here we can frame the prompt and format it is such way that the language model is able to easier process it the way we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9fce337-de2a-4c1f-a633-adfbe502c1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"Question: You're a friendly and to the point assistant ready to act as an encyclopedia of knowledge. My question is: {question}\n",
    "\n",
    "Answer: Let's get straight to the point.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086b2015-b759-41e9-a417-b709e1251ea0",
   "metadata": {},
   "source": [
    "## Loading the model\n",
    "With `LlamaCpp` we can load the module.\n",
    "\n",
    "```\n",
    "    lora_base: Default is None. This is the path to the Llama LoRA base model.\n",
    "    lora_path: Default is None. This is the path to the Llama LoRA. If None, no LoRa is loaded.\n",
    "    n_ctx: Default is 512. This is the token context window.\n",
    "    n_parts: Default is -1. This is the number of parts to split the model into. If -1, the number of parts is automatically determined.\n",
    "    seed: Default is -1. This is the seed for random number generation. If -1, a random seed is used.\n",
    "    f16_kv: Default is True. This decides whether to use half-precision for key/value cache.\n",
    "    logits_all: Default is False. This decides whether to return logits for all tokens, not just the last token.\n",
    "    vocab_only: Default is False. This decides whether to only load the vocabulary, no weights.\n",
    "    use_mlock: Default is False. This forces the system to keep the model in RAM.\n",
    "    n_threads: Default is None. This is the number of threads to use. If None, the number of threads is automatically determined.\n",
    "    n_batch: Default is 8. This is the number of tokens to process in parallel. Should be a number between 1 and n_ctx.\n",
    "    n_gpu_layers: Default is None. This is the number of layers to be loaded into GPU memory.\n",
    "    suffix: Default is None. This is a suffix to append to the generated text. If None, no suffix is appended.\n",
    "    max_tokens: Default is 256. This is the maximum number of tokens to generate.\n",
    "    temperature: Default is 0.8. This is the temperature to use for sampling.\n",
    "    top_p: Default is 0.95. This is the top-p value to use for sampling.\n",
    "    logprobs: Default is None. This is the number of logprobs to return. If None, no logprobs are returned.\n",
    "    echo: Default is False. This decides whether to echo the prompt.\n",
    "    stop: Default is an empty list []. This is a list of strings to stop generation when encountered.\n",
    "    repeat_penalty: Default is 1.1. This is the penalty to apply to repeated tokens.\n",
    "    top_k: Default is 40. This is the top-k value to use for sampling.\n",
    "    last_n_tokens_size: Default is 64. This is the number of tokens to look back when applying the repeat_penalty.\n",
    "    use_mmap: Default is True. This decides whether to keep the model loaded in RAM.\n",
    "    streaming: Default is True. This decides whether to stream the results, token by token.\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8099bfb-99a3-4ac7-9522-2ca0d582c3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# Callbacks support token-wise streaming\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "# Verbose is required to pass to the callback manager\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=my_model_path, \n",
    "    callback_manager=callback_manager, \n",
    "    verbose=False,\n",
    "    temperature=0.6,\n",
    "    seed=my_seed,\n",
    "    max_tokens=350,\n",
    "    n_ctx=2048,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00770b92-90aa-4170-8f94-7eeaf10da4c2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m LLMChain(prompt\u001b[38;5;241m=\u001b[39mprompt, llm\u001b[38;5;241m=\u001b[39mllm)\n\u001b[1;32m      3\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexplain work with a moodboard with my team\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m predict \u001b[38;5;241m=\u001b[39m \u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m predict)\n",
      "File \u001b[0;32m/opt/conda/envs/rob/lib/python3.9/site-packages/langchain/chains/llm.py:213\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m/opt/conda/envs/rob/lib/python3.9/site-packages/langchain/chains/base.py:140\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    141\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[0;32m/opt/conda/envs/rob/lib/python3.9/site-packages/langchain/chains/base.py:134\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks)\u001b[0m\n\u001b[1;32m    128\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    129\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    130\u001b[0m     inputs,\n\u001b[1;32m    131\u001b[0m )\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 134\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    137\u001b[0m     )\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    139\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/conda/envs/rob/lib/python3.9/site-packages/langchain/chains/llm.py:69\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     66\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m     67\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     68\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m---> 69\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/rob/lib/python3.9/site-packages/langchain/chains/llm.py:79\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m     78\u001b[0m prompts, stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_prompts(input_list, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m     81\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/rob/lib/python3.9/site-packages/langchain/llms/base.py:134\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    129\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m    130\u001b[0m     stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    131\u001b[0m     callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    132\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    133\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/rob/lib/python3.9/site-packages/langchain/llms/base.py:191\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    190\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 191\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    192\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_llm_end(output)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/opt/conda/envs/rob/lib/python3.9/site-packages/langchain/llms/base.py:185\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[1;32m    180\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    181\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m}, prompts, invocation_params\u001b[38;5;241m=\u001b[39mparams\n\u001b[1;32m    182\u001b[0m )\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    184\u001b[0m     output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 185\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    188\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    190\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[0;32m/opt/conda/envs/rob/lib/python3.9/site-packages/langchain/llms/base.py:405\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager)\u001b[0m\n\u001b[1;32m    402\u001b[0m new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m    404\u001b[0m     text \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(prompt, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    408\u001b[0m     )\n\u001b[1;32m    409\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([Generation(text\u001b[38;5;241m=\u001b[39mtext)])\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m/opt/conda/envs/rob/lib/python3.9/site-packages/langchain/llms/llamacpp.py:225\u001b[0m, in \u001b[0;36mLlamaCpp._call\u001b[0;34m(self, prompt, stop, run_manager)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming:\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If streaming is enabled, we use the stream\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# method that yields as they are generated\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# and return the combined strings from the first choices's text:\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     combined_text_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 225\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(prompt\u001b[38;5;241m=\u001b[39mprompt, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager):\n\u001b[1;32m    226\u001b[0m         combined_text_output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m token[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined_text_output\n",
      "File \u001b[0;32m/opt/conda/envs/rob/lib/python3.9/site-packages/langchain/llms/llamacpp.py:274\u001b[0m, in \u001b[0;36mLlamaCpp.stream\u001b[0;34m(self, prompt, stop, run_manager)\u001b[0m\n\u001b[1;32m    272\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_parameters(stop)\n\u001b[1;32m    273\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient(prompt\u001b[38;5;241m=\u001b[39mprompt, stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[1;32m    275\u001b[0m     token \u001b[38;5;241m=\u001b[39m chunk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    276\u001b[0m     log_probs \u001b[38;5;241m=\u001b[39m chunk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchoices\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/envs/rob/lib/python3.9/site-packages/llama_cpp/llama.py:667\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model)\u001b[0m\n\u001b[1;32m    665\u001b[0m finish_reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    666\u001b[0m multibyte_fix \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 667\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    668\u001b[0m     prompt_tokens,\n\u001b[1;32m    669\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    670\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m    671\u001b[0m     temp\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m    672\u001b[0m     tfs_z\u001b[38;5;241m=\u001b[39mtfs_z,\n\u001b[1;32m    673\u001b[0m     mirostat_mode\u001b[38;5;241m=\u001b[39mmirostat_mode,\n\u001b[1;32m    674\u001b[0m     mirostat_tau\u001b[38;5;241m=\u001b[39mmirostat_tau,\n\u001b[1;32m    675\u001b[0m     mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[1;32m    676\u001b[0m     frequency_penalty\u001b[38;5;241m=\u001b[39mfrequency_penalty,\n\u001b[1;32m    677\u001b[0m     presence_penalty\u001b[38;5;241m=\u001b[39mpresence_penalty,\n\u001b[1;32m    678\u001b[0m     repeat_penalty\u001b[38;5;241m=\u001b[39mrepeat_penalty,\n\u001b[1;32m    679\u001b[0m ):\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m==\u001b[39m Llama\u001b[38;5;241m.\u001b[39mtoken_eos():\n\u001b[1;32m    681\u001b[0m         text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize(completion_tokens)\n",
      "File \u001b[0;32m/opt/conda/envs/rob/lib/python3.9/site-packages/llama_cpp/llama.py:520\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta)\u001b[0m\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 520\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    521\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    522\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    523\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    531\u001b[0m         mirostat_eta\u001b[38;5;241m=\u001b[39mmirostat_eta,\n\u001b[1;32m    532\u001b[0m     )\n\u001b[1;32m    533\u001b[0m     tokens_or_none \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m token\n",
      "File \u001b[0;32m/opt/conda/envs/rob/lib/python3.9/site-packages/llama_cpp/llama.py:259\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    257\u001b[0m n_past \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n_ctx \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_tokens))\n\u001b[1;32m    258\u001b[0m n_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[0;32m--> 259\u001b[0m return_code \u001b[38;5;241m=\u001b[39m \u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_token\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_past\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mint\u001b[39m(return_code) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_eval returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/rob/lib/python3.9/site-packages/llama_cpp/llama_cpp.py:341\u001b[0m, in \u001b[0;36mllama_eval\u001b[0;34m(ctx, tokens, n_tokens, n_past, n_threads)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mllama_eval\u001b[39m(\n\u001b[1;32m    335\u001b[0m     ctx: llama_context_p,\n\u001b[1;32m    336\u001b[0m     tokens,  \u001b[38;5;66;03m# type: Array[llama_token]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    339\u001b[0m     n_threads: c_int,\n\u001b[1;32m    340\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m c_int:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_past\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"explain work with a moodboard with my team\"\n",
    "\n",
    "predict = llm_chain.predict(question=question)\n",
    "print(\"Prediction: \" + predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6c651a-aaf6-47a6-ac4a-46fb2ed45825",
   "metadata": {},
   "source": [
    "# DuckDuckGo - Applying an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59e3f165-51df-4243-a6ed-d65c0b78c7fe",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: duckduckgo-search in /opt/conda/envs/rob/lib/python3.9/site-packages (3.0.2)\n",
      "Requirement already satisfied: click>=8.1.3 in /opt/conda/envs/rob/lib/python3.9/site-packages (from duckduckgo-search) (8.1.3)\n",
      "Requirement already satisfied: requests>=2.30.0 in /opt/conda/envs/rob/lib/python3.9/site-packages (from duckduckgo-search) (2.30.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/rob/lib/python3.9/site-packages (from requests>=2.30.0->duckduckgo-search) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/rob/lib/python3.9/site-packages (from requests>=2.30.0->duckduckgo-search) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/rob/lib/python3.9/site-packages (from requests>=2.30.0->duckduckgo-search) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/rob/lib/python3.9/site-packages (from requests>=2.30.0->duckduckgo-search) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1faa029f-4143-4669-b337-fb1b7b9f6737",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.agents import Tool, AgentExecutor, BaseSingleActionAgent\n",
    "from langchain.tools import DuckDuckGoSearchRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "972f3b5f-b7b9-4d38-8a60-7f183efccce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "search = DuckDuckGoSearchRun()\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"Intermediate Answer\", # Depending on the AgentType this changes. I am using SELF_ASK_WITH_SEARCH rn.\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events\",\n",
    "        return_direct=True\n",
    "    ),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85e1f984-6db8-475d-bc45-45b480f0cceb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "researcher = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0edfd70f-d23d-4383-8258-a87ca0c7b8c5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "self_ask_with_search = initialize_agent(tools, llm, agent=AgentType.SELF_ASK_WITH_SEARCH, verbose=True)\n",
    "# self_ask_with_search.run(\"What is the hometown of the reigning men's U.S. Open champion?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66b84d0a-9701-42d9-8f91-aef61655821d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcc4182-a78c-49bf-9913-3aa956a7b893",
   "metadata": {},
   "source": [
    "# Journalistic AutoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4be95827-111b-480f-834b-bd2f3f1bdf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the editorial staff.\n",
    "\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "# The journalist\n",
    "template = \"\"\"You are a journalist from Vice. Given this search result, it is your job to write an intriguing and engaging expose about it.\n",
    "\n",
    "Search result: {search_results}\n",
    "Journalist: This is the expose:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"search_results\"], template=template)\n",
    "journalist = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt_template, \n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "# The editor\n",
    "template = \"\"\"You are an editor from Vice. Given this article, it is your job suggestions improvement.\n",
    "\n",
    "Research material: {proposal}\n",
    "Editor: This is the suggestions:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"proposal\"], template=template)\n",
    "editor = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt_template, \n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "# The journalist\n",
    "template = \"\"\"You are a journalist from Vice. Given these suggestions from the editor it is your just to decide which you want to implement.\n",
    "\n",
    "Suggestion from editor: {suggestions}\n",
    "Journalist: This is the new improved version according to suggestions from editor:\"\"\"\n",
    "prompt_template = PromptTemplate(input_variables=[\"suggestions\"], template=template)\n",
    "second_journalist = LLMChain(\n",
    "    llm=llm, \n",
    "    prompt=prompt_template, \n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d35d8bf-7943-4149-bdee-911c4461bdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[self_ask_with_search, journalist, editor, second_journalist], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6e0bdb-ef18-4112-940f-2d8785de121c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      " Yes, it could be helpful to know what specifically you want me to focus on in terms of positive effects.\n",
      "Follow up: Could you provide some examples of specific positive effects of drinking tea versus coffee?\u001b[32;1m\u001b[1;3m Yes, it could be helpful to know what specifically you want me to focus on in terms of positive effects.\n",
      "Follow up: Could you provide some examples of specific positive effects of drinking tea versus coffee?\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rob/lib/python3.9/site-packages/duckduckgo_search/compat.py:20: UserWarning: ddg is deprecated. Use DDGS().text() generator\n",
      "  warnings.warn(\"ddg is deprecated. Use DDGS().text() generator\")\n",
      "/opt/conda/envs/rob/lib/python3.9/site-packages/duckduckgo_search/compat.py:22: UserWarning: parameter time is deprecated, use parameter timelimit\n",
      "  warnings.warn(\"parameter time is deprecated, use parameter timelimit\")\n",
      "/opt/conda/envs/rob/lib/python3.9/site-packages/duckduckgo_search/compat.py:24: UserWarning: parameter page is deprecated, use DDGS().text() generator\n",
      "  warnings.warn(\"parameter page is deprecated, use DDGS().text() generator\")\n",
      "/opt/conda/envs/rob/lib/python3.9/site-packages/duckduckgo_search/compat.py:26: UserWarning: parameter max_results is deprecated, use DDGS().text()\n",
      "  warnings.warn(\"parameter max_results is deprecated, use DDGS().text()\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Intermediate answer: \u001b[36;1m\u001b[1;3mCoffee drinkers can raise a mug to fiber, microbiome health and lowering risk for cancer and diabetes. But tea drinkers, do not despair. Tea is undoubtedly good for your blood pressure ... Caffeine in tea: While the exact amount of caffeine can vary depending on the specific type of tea you're sipping, it generally contains a much lower amount than coffee. For example, a cup of green tea 21 contains around 29 mg of caffeine, while the amount of caffeine in black tea 22 is slightly lower, at around 26 mg per cup. Some laboratory studies have shown that tea polyphenols can inhibit the growth of cancer cells; however, human studies have yielded mixed results. [5] 3. More Durable Energy Boost. While tea contains less caffeine than coffee when consumed in the same amount of servings, tea is high in L-theanine, which is a powerful antioxidant that also ... Coffee and tea both contain caffeine. A standard 8-ounce cup of coffee contains about 95 milligrams of caffeine, while a cup of black tea has about 50 milligrams, and green tea has about 30 milligrams. 4 5 That means coffee has twice as much caffeine as the same amount of black tea, and over three times the amount of caffeine as green tea. The Caffeine in Coffee vs Tea. Caffeine is the common denominator when it comes to coffee vs tea. Both beverages contain caffeine, which can have both positive and negative effects on the body. Coffee is well-known for its caffeine content. One cup of regular coffee has 95 milligrams of caffeine.\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mCoffee drinkers can raise a mug to fiber, microbiome health and lowering risk for cancer and diabetes. But tea drinkers, do not despair. Tea is undoubtedly good for your blood pressure ... Caffeine in tea: While the exact amount of caffeine can vary depending on the specific type of tea you're sipping, it generally contains a much lower amount than coffee. For example, a cup of green tea 21 contains around 29 mg of caffeine, while the amount of caffeine in black tea 22 is slightly lower, at around 26 mg per cup. Some laboratory studies have shown that tea polyphenols can inhibit the growth of cancer cells; however, human studies have yielded mixed results. [5] 3. More Durable Energy Boost. While tea contains less caffeine than coffee when consumed in the same amount of servings, tea is high in L-theanine, which is a powerful antioxidant that also ... Coffee and tea both contain caffeine. A standard 8-ounce cup of coffee contains about 95 milligrams of caffeine, while a cup of black tea has about 50 milligrams, and green tea has about 30 milligrams. 4 5 That means coffee has twice as much caffeine as the same amount of black tea, and over three times the amount of caffeine as green tea. The Caffeine in Coffee vs Tea. Caffeine is the common denominator when it comes to coffee vs tea. Both beverages contain caffeine, which can have both positive and negative effects on the body. Coffee is well-known for its caffeine content. One cup of regular coffee has 95 milligrams of caffeine.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a journalist from Vice. Given this search result, it is your job to write an intriguing and engaging expose about it.\n",
      "\n",
      "Search result: Coffee drinkers can raise a mug to fiber, microbiome health and lowering risk for cancer and diabetes. But tea drinkers, do not despair. Tea is undoubtedly good for your blood pressure ... Caffeine in tea: While the exact amount of caffeine can vary depending on the specific type of tea you're sipping, it generally contains a much lower amount than coffee. For example, a cup of green tea 21 contains around 29 mg of caffeine, while the amount of caffeine in black tea 22 is slightly lower, at around 26 mg per cup. Some laboratory studies have shown that tea polyphenols can inhibit the growth of cancer cells; however, human studies have yielded mixed results. [5] 3. More Durable Energy Boost. While tea contains less caffeine than coffee when consumed in the same amount of servings, tea is high in L-theanine, which is a powerful antioxidant that also ... Coffee and tea both contain caffeine. A standard 8-ounce cup of coffee contains about 95 milligrams of caffeine, while a cup of black tea has about 50 milligrams, and green tea has about 30 milligrams. 4 5 That means coffee has twice as much caffeine as the same amount of black tea, and over three times the amount of caffeine as green tea. The Caffeine in Coffee vs Tea. Caffeine is the common denominator when it comes to coffee vs tea. Both beverages contain caffeine, which can have both positive and negative effects on the body. Coffee is well-known for its caffeine content. One cup of regular coffee has 95 milligrams of caffeine.\n",
      "Journalist: This is the expose:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \"Can Your Cup of Joe or Tea Be a Health Boost? - The debate between coffee and tea drinkers has been brewing for years, but recent research suggests that both beverages can have significant health benefits.\" \n",
      "\n",
      "The article goes on to explain how coffee and tea are similar in their caffeine content, with coffee containing more than tea. However, the article highlights studies that suggest that coffee drinkers may have a lower risk of developing certain types of cancer and diabetes. Tea drinkers also have some health benefits, including potentially lowering blood pressure and reducing the risk of heart disease.\n",
      "\n",
      "The article goes on to discuss how both beverages can contribute to gut health, with studies suggesting that coffee may improve microbiome diversity and tea may promote the growth of beneficial bacteria. The article also highlights some potential drawbacks to consuming too much caffeine, including anxiety and insomnia.\n",
      "\n",
      "Overall, the article concludes that both coffee and tea can be healthy choices when consumed in moderation, with different benefits for each drink.\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m \"Can Your Cup of Joe or Tea Be a Health Boost? - The debate between coffee and tea drinkers has been brewing for years, but recent research suggests that both beverages can have significant health benefits.\" \n",
      "\n",
      "The article goes on to explain how coffee and tea are similar in their caffeine content, with coffee containing more than tea. However, the article highlights studies that suggest that coffee drinkers may have a lower risk of developing certain types of cancer and diabetes. Tea drinkers also have some health benefits, including potentially lowering blood pressure and reducing the risk of heart disease.\n",
      "\n",
      "The article goes on to discuss how both beverages can contribute to gut health, with studies suggesting that coffee may improve microbiome diversity and tea may promote the growth of beneficial bacteria. The article also highlights some potential drawbacks to consuming too much caffeine, including anxiety and insomnia.\n",
      "\n",
      "Overall, the article concludes that both coffee and tea can be healthy choices when consumed in moderation, with different benefits for each drink.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are an editor from Vice. Given this article, it is your job suggestions improvement.\n",
      "\n",
      "Research material:  \"Can Your Cup of Joe or Tea Be a Health Boost? - The debate between coffee and tea drinkers has been brewing for years, but recent research suggests that both beverages can have significant health benefits.\" \n",
      "\n",
      "The article goes on to explain how coffee and tea are similar in their caffeine content, with coffee containing more than tea. However, the article highlights studies that suggest that coffee drinkers may have a lower risk of developing certain types of cancer and diabetes. Tea drinkers also have some health benefits, including potentially lowering blood pressure and reducing the risk of heart disease.\n",
      "\n",
      "The article goes on to discuss how both beverages can contribute to gut health, with studies suggesting that coffee may improve microbiome diversity and tea may promote the growth of beneficial bacteria. The article also highlights some potential drawbacks to consuming too much caffeine, including anxiety and insomnia.\n",
      "\n",
      "Overall, the article concludes that both coffee and tea can be healthy choices when consumed in moderation, with different benefits for each drink.\n",
      "Editor: This is the suggestions:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "- To clarify the differences between coffee and tea, it may be helpful to provide a brief overview of their origins and how they are prepared. This could help readers better understand why these two popular beverages have different health benefits.\n",
      "- It would also be beneficial to include specific examples of studies that demonstrate the potential health benefits of coffee and tea consumption. This could help readers better understand the science behind these claims and make more informed decisions about their own consumption habits.\n",
      "- Finally, it may be helpful to provide some guidance on how much coffee or tea is considered safe for consumption. While there is no one-size-fits-all answer, providing some general guidel"
     ]
    }
   ],
   "source": [
    "review = overall_chain.run(\"write an article about the positive effects of drinking tea vs coffee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52e936f-8f31-40b7-97cd-7ec04116bd8d",
   "metadata": {},
   "source": [
    "## Comments & Todo\n",
    "This solution is still quite unstable. It would benefit from a larger context, splitting up the tokens and other such optimizations. It's of course able to perform the task using the `OpenAI` interface but this defies the purpose. Moreover that OpenAI suffers from the same lack of larger context. Until *storywriter* is available for `LlamaCpp` any solution is suboptimal afaik. Now, it is possible to run Storywriter with other toolings than `LlamaCpp` but that's beyond the scope of this experimentation. \n",
    "\n",
    "It seems to need to be ran a few times before it understands how to use the search tool.\n",
    "\n",
    "`n_ctx` was key for `LlamaCpp` but max is 2048. But this is workable for now. I've chained several now having it editorialize eachother."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
